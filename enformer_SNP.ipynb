{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7687e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jovyan/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from Bio.Seq import Seq\n",
    "from datasets import load_dataset\n",
    "from gpn.data import Genome, load_dataset_from_file_or_dir\n",
    "import grelu.resources\n",
    "from grelu.sequence.format import strings_to_one_hot\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "class VEPModel(torch.nn.Module):\n",
    "\tdef __init__(self, model, personalized_enformer=False, personalized_enformer_diff=False, shifts=[0]):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.model = model\n",
    "\t\tself.shifts = shifts\n",
    "\t\tself.personalized_enformer = personalized_enformer\n",
    "\t\tself.personalized_enformer_diff = personalized_enformer_diff\n",
    "\n",
    "\tdef get_personalized_enformer_diff_scores(self, x_ref, x_alt):\n",
    "\t\tx_ref = x_ref.permute(0, 2, 1)\n",
    "\t\tx_alt = x_alt.permute(0, 2, 1)\n",
    "\t\tseq1 = torch.cat([x_ref.unsqueeze(1), x_ref.unsqueeze(1)], dim=1)\n",
    "\t\tseq2 = torch.cat([x_alt.unsqueeze(1), x_alt.unsqueeze(1)], dim=1)\n",
    "\t\tbatch = {\"seq1\":seq1, \"seq2\":seq2}\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = self.model.predict_step(batch, batch_idx=0)\n",
    "\t\treturn outputs[\"Y_diff\"]\n",
    "\n",
    "\tdef get_personalized_enformer_scores(self, x_ref, x_alt):\n",
    "\t\tx_ref = x_ref.permute(0, 2, 1)\n",
    "\t\tx_alt = x_alt.permute(0, 2, 1)\n",
    "\n",
    "\t\toutputs_ref = self.model(x_ref, \n",
    "               return_base_predictions=True,\n",
    "\t\t\t   base_predictions_head=None)\n",
    "\t\toutputs_alt = self.model(x_alt, \n",
    "               return_base_predictions=True,\n",
    "\t\t\t   base_predictions_head=None)\n",
    "\t\ty_ref = torch.cat([outputs_ref['human'], outputs_ref['mouse']], dim=2)\n",
    "\t\tassert y_ref.shape[1:] == (896, 5313 + 1643), f\"y_ref.shape: {y_ref.shape}; outputs_ref['human'].shape: {outputs_ref['human'].shape}; outputs_ref['mouse'].shape: {outputs_ref['mouse'].shape}\"\n",
    "\t\ty_alt = torch.cat([outputs_alt['human'], outputs_alt['mouse']], dim=2)\n",
    "\t\tassert y_alt.shape[1:] == (896, 5313 + 1643), f\"y_alt.shape: {y_alt.shape}; outputs_alt['human'].shape: {outputs_alt['human'].shape}; outputs_alt['mouse'].shape: {outputs_alt['mouse'].shape}\"\n",
    "\t\tlfc = torch.log2(1 + y_alt) - torch.log2(1 + y_ref) # torch.Size([bs, 896, 5313 + 1643]) for human\n",
    "\t\tl2 = torch.linalg.norm(lfc, dim=1) # [bs, 5313 + 1643]\n",
    "\t\treturn l2\n",
    "\n",
    "\tdef get_scores(self, x_ref, x_alt):\n",
    "\t\ty_ref = self.model(x_ref)\n",
    "\t\ty_alt = self.model(x_alt)\n",
    "\t\tlfc = torch.log2(1 + y_alt) - torch.log2(1 + y_ref) # [bs, 5313, 896]\n",
    "\t\tl2 = torch.linalg.norm(lfc, dim=2) # [bs, 5313]\n",
    "\t\treturn l2\n",
    "\n",
    "\tdef shift(self, x, shift_size):\n",
    "\t\tif shift_size == 0:\n",
    "\t\t\treturn x\n",
    "\t\t\n",
    "\t\toriginal_shape = x.shape\n",
    "\t\tassert x.shape[1] == 4, f\"x.shape: {x.shape}\"\n",
    "\n",
    "\t\t# x is bs x 4 x seq_len torch.Size([bs, 4, 196608])\n",
    "\t\t# we want to shift x by shift_size adding padding to the left or right\n",
    "\n",
    "\t\tNs = torch.zeros(x.shape[0], 4, abs(shift_size), device=x.device, dtype=x.dtype)+0.25\n",
    "\t\tassert Ns.shape == (x.shape[0], 4, abs(shift_size)), f\"Ns.shape: {Ns.shape}, shift_size: {shift_size}, x.shape: {x.shape}\"\n",
    "\n",
    "\t\tif shift_size > 0:\n",
    "\t\t\t#0: torch.Size([4, 4, 196608])\n",
    "\t\t\t# 1: torch.Size([4, 4, 196611])\n",
    "\t\t\t# 2: torch.Size([4, 4, 3])\n",
    "\n",
    "\t\t\t# add padding to the left\n",
    "\t\t\t# print (\"0:\",x.shape)\n",
    "\t\t\tx = torch.cat([Ns, x], dim=2)\n",
    "\t\t\t# print (\"1:\",x.shape)\n",
    "\t\t\tx = x[:,:,:-shift_size]\n",
    "\t\t\t# print (\"2:\",x.shape, shift_size)\n",
    "\t\telse:\n",
    "\t\t\t# add padding to the right\n",
    "\t\t\t# print (\"0:\",x.shape)\n",
    "\t\t\tx = torch.cat([x, Ns], dim=2)\n",
    "\t\t\t# print (\"1:\",x.shape)\n",
    "\t\t\tx = x[:,:,-shift_size:]\n",
    "\t\t\t# print (\"2:\",x.shape, shift_size)\n",
    "\t\t\n",
    "\t\tassert x.shape == original_shape, f\"x.shape: {x.shape}, original_shape: {original_shape}, shift_size: {shift_size}\"\n",
    "\t\t# AssertionError: x.shape: torch.Size([4, 2, 196610]),\n",
    "\t\t# original_shape: torch.Size([4, 4, 196608])\n",
    "\n",
    "\t\tassert x.shape[1] == 4, f\"x.shape: {x.shape}\"\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tx_ref_fwd=None,\n",
    "\t\tx_alt_fwd=None,\n",
    "\t\tx_ref_rev=None,\n",
    "\t\tx_alt_rev=None,\n",
    "\t):\n",
    "\t\tif self.personalized_enformer:\n",
    "\t\t\tscore_fn = self.get_personalized_enformer_scores\n",
    "\t\telif self.personalized_enformer_diff:\n",
    "\t\t\tscore_fn = self.get_personalized_enformer_diff_scores\n",
    "\t\telse:\n",
    "\t\t\tscore_fn = self.get_scores\n",
    "\n",
    "\t\tscores = []\n",
    "\t\tfor shift in self.shifts:\n",
    "\t\t\tscores.append(score_fn(self.shift(x_ref_fwd, shift), self.shift(x_alt_fwd, shift)))\n",
    "\t\t\tscores.append(score_fn(self.shift(x_ref_rev, shift), self.shift(x_alt_rev, shift)))\n",
    "\n",
    "\t\treturn sum(scores)/len(scores)\n",
    "\n",
    "\n",
    "def run_vep(\n",
    "\tvariants,\n",
    "\tgenome,\n",
    "\twindow_size,\n",
    "\tmodel,\n",
    "\tper_device_batch_size=8,\n",
    "\tdataloader_num_workers=0,\n",
    "):\n",
    "\tdef transform(V):\n",
    "\t\t# we convert from 1-based coordinate (standard in VCF) to\n",
    "\t\t# 0-based, to use with Genome\n",
    "\t\tchrom = np.array(V[\"chrom\"])\n",
    "\t\tn = len(chrom)\n",
    "\t\tpos = np.array(V[\"pos\"]) - 1\n",
    "\t\tstart = pos - window_size // 2\n",
    "\t\tend = pos + window_size // 2\n",
    "\t\tseq_fwd, seq_rev = zip(\n",
    "\t\t\t*(genome.get_seq_fwd_rev(chrom[i], start[i], end[i]) for i in range(n))\n",
    "\t\t)\n",
    "\t\tseq_fwd = np.array([list(seq.upper()) for seq in seq_fwd], dtype=\"object\")\n",
    "\t\tseq_rev = np.array([list(seq.upper()) for seq in seq_rev], dtype=\"object\")\n",
    "\t\tassert seq_fwd.shape[1] == window_size\n",
    "\t\tassert seq_rev.shape[1] == window_size\n",
    "\t\tref_fwd = np.array(V[\"ref\"])\n",
    "\t\talt_fwd = np.array(V[\"alt\"])\n",
    "\t\tref_rev = np.array([str(Seq(x).reverse_complement()) for x in ref_fwd])\n",
    "\t\talt_rev = np.array([str(Seq(x).reverse_complement()) for x in alt_fwd])\n",
    "\t\tpos_fwd = window_size // 2\n",
    "\t\tpos_rev = pos_fwd - 1 if window_size % 2 == 0 else pos_fwd\n",
    "\n",
    "\t\tdef prepare_output(seq, pos, ref, alt):\n",
    "\t\t\tassert (seq[:, pos] == ref).all(), f\"{seq[:, pos]}, {ref}\"\n",
    "\t\t\tseq_ref = seq\n",
    "\t\t\tseq_alt = seq.copy()\n",
    "\t\t\tseq_alt[:, pos] = alt\n",
    "\t\t\treturn (\n",
    "\t\t\t\tstrings_to_one_hot([\"\".join(x) for x in seq_ref]),\n",
    "\t\t\t\tstrings_to_one_hot([\"\".join(x) for x in seq_alt]),\n",
    "\t\t\t)\n",
    "\n",
    "\t\tres = {}\n",
    "\t\tres[\"x_ref_fwd\"], res[\"x_alt_fwd\"] = prepare_output(seq_fwd, pos_fwd, ref_fwd, alt_fwd)\n",
    "\t\tres[\"x_ref_rev\"], res[\"x_alt_rev\"] = prepare_output(seq_rev, pos_rev, ref_rev, alt_rev)\n",
    "\t\treturn res\n",
    "\n",
    "\tvariants.set_transform(transform)\n",
    "\ttraining_args = TrainingArguments(\n",
    "\t\toutput_dir=tempfile.TemporaryDirectory().name,\n",
    "\t\tper_device_eval_batch_size=per_device_batch_size,\n",
    "\t\tdataloader_num_workers=dataloader_num_workers,\n",
    "\t\tremove_unused_columns=False,\n",
    "\t\treport_to=\"none\",  # disables all reporting, including wandb\n",
    "\t)\n",
    "\ttrainer = Trainer(model=model, args=training_args)\n",
    "\treturn trainer.predict(test_dataset=variants).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0c201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"complex_traits_matched_9\"\n",
    "if not os.path.exists(f\"results/dataset/{dataset}/test.parquet\"):\n",
    "\tos.makedirs(f\"results/dataset/{dataset}\", exist_ok=True)\n",
    "pd.read_parquet(f\"hf://datasets/songlab/TraitGym/{dataset}/test.parquet\").to_parquet(f\"results/dataset/{dataset}/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951e3c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = Genome(\"results/genome.fa.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c34f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = load_dataset_from_file_or_dir(\n",
    "\t\tf\"results/dataset/{dataset}/test.parquet\",\n",
    "\t\tsplit=\"test\",\n",
    "\t\tis_file=True,\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7682842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variants = variants.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4ce2440c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/pytorch_lightning/utilities/migration/utils.py:49: The loaded checkpoint was produced with Lightning v2.5.0.post0, which is newer than your current Lightning version: v1.9.5\n"
     ]
    }
   ],
   "source": [
    "# model = grelu.resources.load_model(project=\"enformer\", model_name=\"human\")\n",
    "# instead:\n",
    "# download somewhere where wandb is no blocked\n",
    "# import wandb\n",
    "# api = wandb.Api()\n",
    "# art = api.artifact('grelu/enformer/human:latest')\n",
    "# art.download(\"C:\\\\Users\\\\user\\\\Downloads\\\\\")\n",
    "\n",
    "# then upload to /data/ckpts/wandb_human_enformer_latest.ckpt\n",
    "\n",
    "from grelu.lightning import LightningModel\n",
    "cpt_dir = \"data/ckpts/\"\n",
    "cpt_name = \"wandb_human_enformer_latest.ckpt\"\n",
    "model = LightningModel.load_from_checkpoint(os.path.join(cpt_dir, cpt_name), map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ca7a8dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_output_path = \"results/metadata/Enformer.csv\"\n",
    "metadata = pd.DataFrame(model.data_params[\"tasks\"])\n",
    "if not os.path.exists(metadata_output_path):\n",
    "\tos.makedirs(os.path.dirname(metadata_output_path), exist_ok=True)\n",
    "\tmetadata.to_csv(metadata_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "42216401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='357' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/357 00:29 < 1:26:12, 0.07 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m per_device_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      7\u001b[0m dataloader_num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m----> 9\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mrun_vep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mvariants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mgenome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmodel_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mper_device_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_device_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shifts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     19\u001b[0m \toutput_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/dataset/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/features/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpt_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(shifts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_L2.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[103], line 161\u001b[0m, in \u001b[0;36mrun_vep\u001b[0;34m(variants, genome, window_size, model, per_device_batch_size, dataloader_num_workers)\u001b[0m\n\u001b[1;32m    153\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m    154\u001b[0m \toutput_dir\u001b[38;5;241m=\u001b[39mtempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory()\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    155\u001b[0m \tper_device_eval_batch_size\u001b[38;5;241m=\u001b[39mper_device_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \treport_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# disables all reporting, including wandb\u001b[39;00m\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    160\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel, args\u001b[38;5;241m=\u001b[39mtraining_args)\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariants\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredictions\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/transformers/trainer.py:4277\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4274\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4276\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4277\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[1;32m   4279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4280\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/transformers/trainer.py:4394\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4391\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   4393\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 4394\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4395\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4396\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4398\u001b[0m )\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/transformers/trainer.py:4620\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4618\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4619\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4620\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   4622\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore_keys)\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:194\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    193\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 194\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:213\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparallel_apply\u001b[39m(\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[1;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Any]:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:119\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    117\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m--> 119\u001b[0m         \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m], streams[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "columns = model.data_params['tasks'][\"name\"]\n",
    "window_size = model.data_params[\"train\"][\"seq_len\"]\n",
    "shifts = [-3,-6,0,3,6]\n",
    "model_full = VEPModel(model.model, shifts=shifts)\n",
    "\n",
    "per_device_batch_size = 4\n",
    "dataloader_num_workers = 16\n",
    "\n",
    "pred = run_vep(\n",
    "\tvariants,\n",
    "\tgenome,\n",
    "\twindow_size,\n",
    "\tmodel_full,\n",
    "\tper_device_batch_size=per_device_batch_size,\n",
    "\tdataloader_num_workers=dataloader_num_workers,\n",
    ")\n",
    "\n",
    "if len(shifts)>1:\n",
    "\tsave_model_name = f\"{cpt_name}-{len(shifts)}\"\n",
    "else:\n",
    "\tsave_model_name = cpt_name\n",
    "output_path = f\"results/dataset/{dataset}/features/{save_model_name}_L2.parquet\"\n",
    "\n",
    "directory = os.path.dirname(output_path)\n",
    "if directory != \"\" and not os.path.exists(directory):\n",
    "\tos.makedirs(directory)\n",
    "pd.DataFrame(pred, columns=columns).to_parquet(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0375b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy metadata file\n",
    "metadata = pd.read_csv(\"results/metadata/Enformer_human.csv\")\n",
    "metadata.to_csv(f\"results/metadata/{save_model_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31017f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11400, 5313)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape # (11400, 5313) - 11400 SNVs x 5313 enformer track predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81617505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to calc stats:\n",
    "# snakemake -s workflow/Snakefile --cores 4 results/dataset/complex_traits_matched_9/AUPRC_by_chrom_weighted_average/all/Enformer_L2_L2.plus.all.csv\n",
    "\n",
    "# job                                     count\n",
    "# ------------------------------------  -------\n",
    "# dataset_subset_all                          1\n",
    "# get_metric_by_block                         1\n",
    "# get_metric_by_block_weighted_average        1\n",
    "# grelu_aggregate_assay                       1\n",
    "# unsupervised_pred                           1\n",
    "# total                                       5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3509d7",
   "metadata": {},
   "source": [
    "# Explore checkpoints of enformer pretrained on human variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dabebe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = \"joint_regression_data_seed_42_lr_0.0005_wd_0.005_rcprob_0.5_rsmax_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1ab6fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded checkpoint to data/ckpts/joint_regression_data_seed_42_lr_0.0005_wd_0.005_rcprob_0.5_rsmax_3_best.ckpt\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "\n",
    "# url = f\"https://huggingface.co/anikethjr/finetuning-enformer/resolve/main/saved_models/{cpt}/checkpoints/best.ckpt?download=true\"\n",
    "# output_file = f\"data/ckpts/{cpt}_best.ckpt\"\n",
    "\n",
    "# os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# with requests.get(url, stream=True) as r:\n",
    "#     r.raise_for_status()\n",
    "#     with open(output_file, \"wb\") as f:\n",
    "#         for chunk in r.iter_content(chunk_size=8192):\n",
    "#             if chunk:\n",
    "#                 f.write(chunk)\n",
    "# print(f\"Downloaded checkpoint to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00f1d926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/jovyan/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/jovyan/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/jovyan/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    }
   ],
   "source": [
    "cpt_dir = \"data/ckpts/\"\n",
    "cpt_name = f\"{cpt}_best.ckpt\"\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"../finetuning-enformer/\")\n",
    "from finetuning.models import PairwiseRegressionWithOriginalDataJointTrainingFloatPrecision\n",
    "\n",
    "pers_enf_model = PairwiseRegressionWithOriginalDataJointTrainingFloatPrecision.load_from_checkpoint(\n",
    "\tos.path.join(cpt_dir, cpt_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "44f23419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pers_enf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e03e7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 49152 for pairwise regression, 196608 for base enformer outputs\n",
    "window_size = 49152 # 196608\n",
    "\n",
    "def get_transformed_variants(\n",
    "\tvariants,\n",
    "\tgenome,\n",
    "\twindow_size,\n",
    "):\n",
    "\tdef transform(V):\n",
    "\t\t# we convert from 1-based coordinate (standard in VCF) to\n",
    "\t\t# 0-based, to use with Genome\n",
    "\t\tchrom = np.array(V[\"chrom\"])\n",
    "\t\tn = len(chrom)\n",
    "\t\tpos = np.array(V[\"pos\"]) - 1\n",
    "\t\tstart = pos - window_size // 2\n",
    "\t\tend = pos + window_size // 2\n",
    "\t\tseq_fwd, seq_rev = zip(\n",
    "\t\t\t*(genome.get_seq_fwd_rev(chrom[i], start[i], end[i]) for i in range(n))\n",
    "\t\t)\n",
    "\t\tseq_fwd = np.array([list(seq.upper()) for seq in seq_fwd], dtype=\"object\")\n",
    "\t\tseq_rev = np.array([list(seq.upper()) for seq in seq_rev], dtype=\"object\")\n",
    "\t\tassert seq_fwd.shape[1] == window_size\n",
    "\t\tassert seq_rev.shape[1] == window_size\n",
    "\t\tref_fwd = np.array(V[\"ref\"])\n",
    "\t\talt_fwd = np.array(V[\"alt\"])\n",
    "\t\tref_rev = np.array([str(Seq(x).reverse_complement()) for x in ref_fwd])\n",
    "\t\talt_rev = np.array([str(Seq(x).reverse_complement()) for x in alt_fwd])\n",
    "\t\tpos_fwd = window_size // 2\n",
    "\t\tpos_rev = pos_fwd - 1 if window_size % 2 == 0 else pos_fwd\n",
    "\n",
    "\t\tdef prepare_output(seq, pos, ref, alt):\n",
    "\t\t\tassert (seq[:, pos] == ref).all(), f\"{seq[:, pos]}, {ref}\"\n",
    "\t\t\tseq_ref = seq\n",
    "\t\t\tseq_alt = seq.copy()\n",
    "\t\t\tseq_alt[:, pos] = alt\n",
    "\t\t\treturn (\n",
    "\t\t\t\tstrings_to_one_hot([\"\".join(x) for x in seq_ref]),\n",
    "\t\t\t\tstrings_to_one_hot([\"\".join(x) for x in seq_alt]),\n",
    "\t\t\t)\n",
    "\n",
    "\t\tres = {}\n",
    "\t\tres[\"x_ref_fwd\"], res[\"x_alt_fwd\"] = prepare_output(seq_fwd, pos_fwd, ref_fwd, alt_fwd)\n",
    "\t\tres[\"x_ref_rev\"], res[\"x_alt_rev\"] = prepare_output(seq_rev, pos_rev, ref_rev, alt_rev)\n",
    "\t\treturn res\n",
    "\n",
    "\tvariants.set_transform(transform)\n",
    "\treturn variants\n",
    "v = get_transformed_variants(variants.select(range(100)), genome, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49be2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v[0] is a dict with keys:\n",
    "# {'x_ref_fwd': tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
    "#          [0., 1., 0.,  ..., 0., 0., 0.],\n",
    "#          [0., 0., 0.,  ..., 1., 1., 0.],\n",
    "#          [1., 0., 0.,  ..., 0., 0., 1.]]),\n",
    "#  'x_alt_fwd': tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
    "#          [0., 1., 0.,  ..., 0., 0., 0.],\n",
    "#          [0., 0., 0.,  ..., 1., 1., 0.],\n",
    "#          [1., 0., 0.,  ..., 0., 0., 1.]]),\n",
    "#  'x_ref_rev': tensor([[1., 0., 0.,  ..., 0., 0., 1.],\n",
    "#          [0., 1., 1.,  ..., 0., 0., 0.],\n",
    "#          [0., 0., 0.,  ..., 0., 1., 0.],\n",
    "#          [0., 0., 0.,  ..., 1., 0., 0.]]),\n",
    "#  'x_alt_rev': tensor([[1., 0., 0.,  ..., 0., 0., 1.],\n",
    "#          [0., 1., 1.,  ..., 0., 0., 0.],\n",
    "#          [0., 0., 0.,  ..., 0., 1., 0.],\n",
    "#          [0., 0., 0.,  ..., 1., 0., 0.]])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c92c8ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "sequence length 384 is less than target length 896",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m pers_enf_model \u001b[38;5;241m=\u001b[39m pers_enf_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m v[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_ref_fwd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpers_enf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m\t\t\t   \u001b[49m\u001b[43mreturn_base_predictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m\t\t\t   \u001b[49m\u001b[43mbase_predictions_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m outputs\u001b[38;5;241m.\u001b[39msize \u001b[38;5;66;03m# outputs.size\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/DNALM/TraitGym/../finetuning-enformer/finetuning/models.py:1393\u001b[0m, in \u001b[0;36mPairwiseRegressionWithOriginalDataJointTrainingFloatPrecision.forward\u001b[0;34m(self, X, return_base_predictions, base_predictions_head, no_haplotype)\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Y\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1393\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_predictions_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m896\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Y\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/enformer_pytorch/modeling_enformer.py:462\u001b[0m, in \u001b[0;36mEnformer.forward\u001b[0;34m(self, x, target, return_corr_coef, return_embeddings, return_only_embeddings, head, target_length)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_target_length(target_length)\n\u001b[1;32m    461\u001b[0m trunk_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrunk_checkpointed \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_checkpointing \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trunk\n\u001b[0;32m--> 462\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtrunk_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_batch:\n\u001b[1;32m    465\u001b[0m     x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m() ... -> ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.mlspace/envs/tgymMamba/lib/python3.10/site-packages/enformer_pytorch/modeling_enformer.py:205\u001b[0m, in \u001b[0;36mTargetLengthCrop.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seq_len \u001b[38;5;241m<\u001b[39m target_len:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is less than target length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    207\u001b[0m trim \u001b[38;5;241m=\u001b[39m (target_len \u001b[38;5;241m-\u001b[39m seq_len) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: sequence length 384 is less than target length 896"
     ]
    }
   ],
   "source": [
    "# Move the model and input to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pers_enf_model = pers_enf_model.to(device)\n",
    "input_tensor = v[0][\"x_ref_fwd\"].unsqueeze(0).permute(0, 2, 1).to(device)\n",
    "outputs = pers_enf_model(input_tensor, \n",
    "\t\t\t   return_base_predictions=True, \n",
    "\t\t\t   base_predictions_head=\"human\")\n",
    "outputs.size # outputs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9e4aa9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_total_bins: 384\n",
      "length in NT: 49152\n",
      "center start: 187 center end: 197\n"
     ]
    }
   ],
   "source": [
    "print (\"n_total_bins:\", pers_enf_model.hparams.n_total_bins)\n",
    "print (\"length in NT:\", pers_enf_model.hparams.n_total_bins*128)\n",
    "print (\"center start:\", pers_enf_model.center_start, \"center end:\", pers_enf_model.center_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06402038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model and input to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pers_enf_model = pers_enf_model.to(device)\n",
    "input_ref = v[0][\"x_ref_fwd\"].unsqueeze(0).permute(0, 2, 1).to(device)\n",
    "input_alt = v[0][\"x_alt_fwd\"].unsqueeze(0).permute(0, 2, 1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdf5e9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49152, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9c97eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Y1_hat': tensor([4.4787], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " 'Y2_hat': tensor([4.4770], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " 'Y_diff': tensor([0.0018], device='cuda:0', grad_fn=<SubBackward0>)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seq1 is something like self.seqs[i]\n",
    "# and self.seqs is             self.seqs = self.h5_file[\"seqs\"][:]  # (n_seqs, 2, length, 4)\n",
    "\n",
    "batch = {\"seq1\":torch.cat([input_ref.unsqueeze(1), input_ref.unsqueeze(1)], dim=1),\n",
    "\t\t\"seq2\":torch.cat([input_alt.unsqueeze(1), input_alt.unsqueeze(1)], dim=1)\n",
    "\t\t}\n",
    "pers_enf_model.predict_step(batch, batch_idx=0)\n",
    "\n",
    "# batch = {\"seq1\":input_ref, \"seq2\":input_alt}\n",
    "# pers_enf_model.predict_step(batch, batch_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "408a7c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:08<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "compare_2_samples = []\n",
    "compare_ref_alt = []\n",
    "import tqdm\n",
    "for i in tqdm.tqdm(range(20)):\n",
    "\twith torch.no_grad():\n",
    "\t\tinputs1 = v[i][\"x_ref_fwd\"].unsqueeze(0).permute(0, 2, 1).unsqueeze(0).to(device)\n",
    "\t\tinputs2 = v[i+1][\"x_ref_fwd\"].unsqueeze(0).permute(0, 2, 1).unsqueeze(0).to(device)\n",
    "\t\tseq1 = torch.cat([inputs1, inputs1], dim=1)\n",
    "\t\tseq2 = torch.cat([inputs2, inputs2], dim=1)\n",
    "\t\tbatch = {\"seq1\":seq1,\"seq2\":seq2}\n",
    "\t\toutputs = pers_enf_model.predict_step(batch, batch_idx=0)\n",
    "\t\tcompare_2_samples.append(outputs[\"Y_diff\"].item())\n",
    "\n",
    "\t\tinputs1 = v[i][\"x_ref_fwd\"].unsqueeze(0).permute(0, 2, 1).unsqueeze(0).to(device)\n",
    "\t\tinputs2 = v[i][\"x_alt_fwd\"].unsqueeze(0).permute(0, 2, 1).unsqueeze(0).to(device)\n",
    "\t\tseq1 = torch.cat([inputs1, inputs1], dim=1)\n",
    "\t\tseq2 = torch.cat([inputs2, inputs2], dim=1)\n",
    "\t\tbatch = {\"seq1\":seq1,\"seq2\":seq2}\n",
    "\t\toutputs = pers_enf_model.predict_step(batch, batch_idx=0)\n",
    "\t\tcompare_ref_alt.append(outputs[\"Y_diff\"].item())\n",
    "\t\tdel inputs1, inputs2, seq1, seq2, batch, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d8fbf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_2_samples: 0.23892528414726258 compare_ref_alt: -0.014012648165225983\n"
     ]
    }
   ],
   "source": [
    "print (\"compare_2_samples:\", np.mean(compare_2_samples), \"compare_ref_alt:\", np.mean(compare_ref_alt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bcc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pers_enf_model = pers_enf_model.to(device)\n",
    "input_tensor = v[0][\"x_ref_fwd\"].unsqueeze(0).permute(0, 2, 1).to(device)\n",
    "outputs = pers_enf_model(input_tensor, \n",
    "\t\t\t   return_base_predictions=True)\n",
    "outputs # this will be a dict with keys: 'human' and 'mouse'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1f1504",
   "metadata": {},
   "source": [
    "# Enformer base outputs (but enfromer was fine-tunned on personalized variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076175c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_size = 196608\n",
    "per_device_batch_size = 1\n",
    "dataloader_num_workers = 2\n",
    "\n",
    "model_full = VEPModel(pers_enf_model, personalized_enformer=True)\n",
    "\n",
    "pred = run_vep(\n",
    "\tvariants,\n",
    "\tgenome,\n",
    "\twindow_size,\n",
    "\tmodel_full,\n",
    "\tper_device_batch_size=per_device_batch_size,\n",
    "\tdataloader_num_workers=dataloader_num_workers,\n",
    ")\n",
    "\n",
    "assert pred.shape[1] == 5313 + 1643, f\"pred.shape: {pred.shape}\"\n",
    "predictions = {\"human\": pred[:,:5313], \"mouse\": pred[:,5313:]}\n",
    "assert predictions[\"human\"].shape[1] == 5313, f\"human_pred.shape: {predictions['human'].shape}\"\n",
    "assert predictions[\"mouse\"].shape[1] == 1643, f\"mouse_pred.shape: {predictions['mouse'].shape}\"\n",
    "\n",
    "for k in [\"human\", \"mouse\"]:\n",
    "\toutput_path = f\"results/dataset/{dataset}/features/{cpt}-{k}_L2.parquet\"\n",
    "\n",
    "\tdirectory = os.path.dirname(output_path)\n",
    "\tif directory != \"\" and not os.path.exists(directory):\n",
    "\t\tos.makedirs(directory, exist_ok=True)\n",
    "\tpd.DataFrame(predictions[k]).to_parquet(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "90bcede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"mouse\"\n",
    "features = pd.read_parquet(f\"results/dataset/complex_traits_matched_9/features/wandb_human_enformer_latest.ckpt-{species}_L2.parquet\")\n",
    "metadata = pd.read_csv(f\"results/metadata/Enformer_{species}.csv\")\n",
    "features.columns = metadata[\"name\"].values\n",
    "features.head()\n",
    "features.to_parquet(f\"results/dataset/complex_traits_matched_9/features/{cpt}-{species}_L2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b4ac2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for species in [\"human\", \"mouse\"]:\n",
    "\tmetadata_df = pd.read_csv(f\"results/metadata/Enformer_{species}.csv\")\n",
    "\tmetadata_df.to_csv(f\"results/metadata/{cpt}-{species}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "47fe1f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-23 16:52:10--  https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_mouse.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 250458 (245K) [text/plain]\n",
      "Saving to: results/metadata/targets_mouse.txt\n",
      "\n",
      "results/metadata/ta 100%[===================>] 244.59K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-07-23 16:52:10 (1.88 MB/s) - results/metadata/targets_mouse.txt saved [250458/250458]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_mouse.txt -O \\\n",
    "\tresults/metadata/targets_mouse.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47407dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"results/metadata/targets_mouse.txt\", sep=\"\\t\")\n",
    "metadata[\"assay\"] = metadata[\"description\"].apply(lambda x: x.split(\":\")[0])\n",
    "metadata[\"name\"] = metadata[\"identifier\"]\n",
    "metadata[[\"name\", \"assay\"]].to_csv(\"results/metadata/Enformer_mouse.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee6e879",
   "metadata": {},
   "source": [
    "# Personalized enformer diff scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c1dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_size = 49152\n",
    "per_device_batch_size = 1\n",
    "dataloader_num_workers = 8\n",
    "\n",
    "model_full = VEPModel(pers_enf_model, personalized_enformer_diff=True)\n",
    "\n",
    "pred = run_vep(\n",
    "\tvariants,\n",
    "\tgenome,\n",
    "\twindow_size,\n",
    "\tmodel_full,\n",
    "\tper_device_batch_size=per_device_batch_size,\n",
    "\tdataloader_num_workers=dataloader_num_workers,\n",
    ")\n",
    "pred = pd.DataFrame(pred, columns=[\"all\"])\n",
    "output_path = f\"results/dataset/{dataset}/features/{cpt}-diff_L2_L2.parquet\"\n",
    "pred.to_parquet(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa531fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_parquet(f\"results/dataset/complex_traits_matched_9/features/{cpt}-diff_L2_L2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac7cd0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[\"all\"] = pred[\"all\"].abs()\n",
    "pred.to_parquet(f\"results/dataset/complex_traits_matched_9/features/{cpt}-diffabs_L2_L2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7367918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = pd.read_parquet(\"results/dataset/complex_traits_matched_9/test.parquet\")\n",
    "V = pd.concat([V,pred], axis=1).rename(columns={\"all\":\"Y_diff\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca96c15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "bins = np.histogram_bin_edges(V.query(\"label==True\")[\"Y_diff\"].abs(), bins=100)\n",
    "plt.hist(V.query(\"label==True\")[\"Y_diff\"].abs(), bins=bins, alpha=0.5, density=True, label=\"True\")\n",
    "plt.hist(V.query(\"label==False\")[\"Y_diff\"].abs(), bins=bins, alpha=0.5, density=True, label=\"False\")\n",
    "plt.xlim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b6720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgymMamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
